#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Streamlitæ‡‰ç”¨ç¨‹å¼ - è‘¡è„é…’å“è³ªé æ¸¬æ¨¡å‹åˆ†æ
å±•ç¤ºæ¨¡å‹æ¯”éš¨æ©Ÿäº‚çŒœæ›´æº–ç¢ºçš„è­‰æ˜åŠç›¸é—œåˆ†æçµæœ
"""

import os

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
import streamlit as st
from plotly.subplots import make_subplots
from sklearn.metrics import (
    accuracy_score,
    mean_absolute_error,
    mean_squared_error,
    r2_score,
)

# è¨­å®šé é¢é…ç½®
st.set_page_config(
    page_title="è‘¡è„é…’å“è³ªé æ¸¬æ¨¡å‹åˆ†æ",
    page_icon="ğŸ·",
    layout="wide",
    initial_sidebar_state="expanded",
)

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams["font.sans-serif"] = ["Microsoft JhengHei", "SimHei"]
plt.rcParams["axes.unicode_minus"] = False


class StreamlitApp:
    """Streamlitæ‡‰ç”¨ç¨‹å¼ä¸»é¡"""

    def __init__(self):
        self.data = None
        self.models = {}
        self.results = {}

    def load_data(self):
        """è¼‰å…¥è³‡æ–™"""
        try:
            # è¼‰å…¥åŸå§‹è³‡æ–™
            self.data = pd.read_csv("datasets/WineQT.csv")

            # è¼‰å…¥è™•ç†å¾Œçš„è³‡æ–™
            self.processed_data = {
                "with_outliers": pd.read_csv(
                    "processed_data_no_outliers/wine_processed.csv"
                ),
                "without_outliers": pd.read_csv("processed_data/wine_processed.csv"),
            }

            # è¼‰å…¥æ¸¬è©¦è³‡æ–™
            self.X_test = pd.read_csv("processed_data_no_outliers/X_test.csv")
            self.y_test = pd.read_csv("processed_data_no_outliers/y_test.csv").squeeze()

            return True
        except FileNotFoundError as e:
            st.error(f"æ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆ: {e}")
            return False

    def load_models(self):
        """è¼‰å…¥æ¨¡å‹"""
        try:
            # è¼‰å…¥ä¿ç•™ç•°å¸¸å€¼ç‰ˆæœ¬çš„æ¨¡å‹
            model_dir = "results_no_outliers/models"
            if os.path.exists(model_dir):
                self.models["ridge"] = joblib.load(f"{model_dir}/ridge_model.pkl")
                self.models["linear"] = joblib.load(
                    f"{model_dir}/linearregression_model.pkl"
                )
                self.models["lasso"] = joblib.load(f"{model_dir}/lasso_model.pkl")
                self.models["elastic"] = joblib.load(
                    f"{model_dir}/elasticnet_model.pkl"
                )
                return True
            else:
                st.error("æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ")
                return False
        except Exception as e:
            st.error(f"è¼‰å…¥æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False

    def show_data_preprocessing(self):
        """é¡¯ç¤ºè³‡æ–™é è™•ç†è³‡è¨Š"""
        st.header("ğŸ“Š è³‡æ–™é è™•ç†çµæœ")

        col1, col2 = st.columns(2)

        with col1:
            st.subheader("åŸå§‹è³‡æ–™çµ±è¨ˆ")
            st.dataframe(self.data.describe(), use_container_width=True)

            st.subheader("è³‡æ–™å“è³ªæª¢æŸ¥")
            quality_info = {
                "ç¸½æ¨£æœ¬æ•¸": len(self.data),
                "ç‰¹å¾µæ•¸": len(self.data.columns) - 1,
                "ç¼ºå¤±å€¼": self.data.isnull().sum().sum(),
                "é‡è¤‡å€¼": self.data.duplicated().sum(),
                "å“è³ªåˆ†æ•¸ç¯„åœ": f"{self.data['quality'].min()} - {self.data['quality'].max()}",
            }

            for key, value in quality_info.items():
                st.metric(key, value)

        with col2:
            st.subheader("è³‡æ–™åˆ†å¸ƒ")

            # å“è³ªåˆ†æ•¸åˆ†å¸ƒ
            fig = px.histogram(
                self.data,
                x="quality",
                title="å“è³ªåˆ†æ•¸åˆ†å¸ƒ",
                color_discrete_sequence=["#1f77b4"],
            )
            fig.update_layout(
                xaxis_title="å“è³ªåˆ†æ•¸", yaxis_title="é »ç‡", showlegend=False
            )
            st.plotly_chart(fig, use_container_width=True)

        # é è™•ç†æ¯”è¼ƒ
        st.subheader("é è™•ç†ç‰ˆæœ¬æ¯”è¼ƒ")

        comparison_data = {
            "ç‰ˆæœ¬": ["ä¿ç•™ç•°å¸¸å€¼", "ç§»é™¤ç•°å¸¸å€¼"],
            "æ¨£æœ¬æ•¸": [
                len(self.processed_data["with_outliers"]),
                len(self.processed_data["without_outliers"]),
            ],
            "ç‰¹å¾µæ•¸": [
                len(self.processed_data["with_outliers"].columns) - 1,
                len(self.processed_data["without_outliers"].columns) - 1,
            ],
            "å“è³ªåˆ†æ•¸ç¯„åœ": [
                f"{self.processed_data['with_outliers']['quality'].min()}-{self.processed_data['with_outliers']['quality'].max()}",
                f"{self.processed_data['without_outliers']['quality'].min()}-{self.processed_data['without_outliers']['quality'].max()}",
            ],
        }

        st.dataframe(pd.DataFrame(comparison_data), use_container_width=True)

    def show_model_performance(self):
        """é¡¯ç¤ºæ¨¡å‹æ•ˆèƒ½"""
        st.header("ğŸ¯ æ¨¡å‹æ•ˆèƒ½åˆ†æ")

        # è¼‰å…¥æ¨¡å‹çµæœ
        try:
            # è¼‰å…¥ä¿ç•™ç•°å¸¸å€¼ç‰ˆæœ¬çš„çµæœ
            results_file = "results_no_outliers/baseline_modeling_results.csv"
            if os.path.exists(results_file):
                results_df = pd.read_csv(results_file)

                col1, col2 = st.columns(2)

                with col1:
                    st.subheader("æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ")
                    st.dataframe(results_df, use_container_width=True)

                with col2:
                    st.subheader("æœ€ä½³æ¨¡å‹")
                    best_model = results_df.loc[results_df["Test_R2"].idxmax()]
                    st.metric("æœ€ä½³æ¨¡å‹", best_model["Model"])
                    st.metric("R2", f"{best_model['Test_R2']:.4f}")
                    st.metric("RMSE", f"{best_model['Test_RMSE']:.4f}")
                    st.metric("MAE", f"{best_model['Test_MAE']:.4f}")

                # æ•ˆèƒ½è¦–è¦ºåŒ–
                st.subheader("æ•ˆèƒ½è¦–è¦ºåŒ–")

                fig = make_subplots(
                    rows=1,
                    cols=3,
                    subplot_titles=("R2 æ¯”è¼ƒ", "RMSE æ¯”è¼ƒ", "MAE æ¯”è¼ƒ"),
                    specs=[[{"type": "bar"}, {"type": "bar"}, {"type": "bar"}]],
                )

                models = results_df["Model"]
                r2_values = results_df["Test_R2"]
                rmse_values = results_df["Test_RMSE"]
                mae_values = results_df["Test_MAE"]

                fig.add_trace(
                    go.Bar(x=models, y=r2_values, name="R2", marker_color="blue"),
                    row=1,
                    col=1,
                )

                fig.add_trace(
                    go.Bar(x=models, y=rmse_values, name="RMSE", marker_color="red"),
                    row=1,
                    col=2,
                )

                fig.add_trace(
                    go.Bar(x=models, y=mae_values, name="MAE", marker_color="green"),
                    row=1,
                    col=3,
                )

                fig.update_layout(
                    height=400, showlegend=False, title_text="æ¨¡å‹æ•ˆèƒ½æ¯”è¼ƒ"
                )

                st.plotly_chart(fig, use_container_width=True)

            else:
                st.warning("æ‰¾ä¸åˆ°æ¨¡å‹çµæœæª”æ¡ˆ")

        except Exception as e:
            st.error(f"è¼‰å…¥æ¨¡å‹çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def show_model_vs_random(self):
        """é¡¯ç¤ºæ¨¡å‹èˆ‡éš¨æ©Ÿé æ¸¬æ¯”è¼ƒ"""
        st.header("ğŸ² æ¨¡å‹ vs éš¨æ©Ÿé æ¸¬æ¯”è¼ƒ")

        if not self.load_models():
            st.error("ç„¡æ³•è¼‰å…¥æ¨¡å‹")
            return

        # ç”Ÿæˆéš¨æ©Ÿé æ¸¬
        np.random.seed(42)
        random_predictions = np.random.uniform(3, 8, size=len(self.y_test))

        # æ¨¡å‹é æ¸¬
        model_predictions = self.models["ridge"].predict(self.X_test)

        # è¨ˆç®—æŒ‡æ¨™
        def calculate_metrics(y_true, y_pred, method_name):
            r2 = r2_score(y_true, y_pred)
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            mae = mean_absolute_error(y_true, y_pred)

            # åˆ†é¡æº–ç¢ºç‡
            y_true_rounded = np.round(y_true).astype(int)
            y_pred_rounded = np.round(y_pred).astype(int)
            y_true_clipped = np.clip(y_true_rounded, 3, 8)
            y_pred_clipped = np.clip(y_pred_rounded, 3, 8)

            exact_accuracy = accuracy_score(y_true_clipped, y_pred_clipped)
            diff = np.abs(y_true_clipped - y_pred_clipped)
            within_one_accuracy = np.mean(diff <= 1)

            return {
                "method": method_name,
                "r2": r2,
                "rmse": rmse,
                "mae": mae,
                "exact_accuracy": exact_accuracy,
                "within_one_accuracy": within_one_accuracy,
            }

        # è¨ˆç®—æ¯”è¼ƒçµæœ
        model_metrics = calculate_metrics(self.y_test, model_predictions, "Ridgeæ¨¡å‹")
        random_metrics = calculate_metrics(self.y_test, random_predictions, "éš¨æ©Ÿé æ¸¬")

        # é¡¯ç¤ºæ¯”è¼ƒè¡¨æ ¼
        st.subheader("è©³ç´°æ¯”è¼ƒçµæœ")
        comparison_df = pd.DataFrame([model_metrics, random_metrics])
        st.dataframe(comparison_df, use_container_width=True)

        # é—œéµæŒ‡æ¨™æ”¹å–„
        st.subheader("é—œéµæŒ‡æ¨™æ”¹å–„")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            r2_improvement = model_metrics["r2"] - random_metrics["r2"]
            st.metric("R2 æ”¹å–„", f"{r2_improvement:.4f}", delta=f"{r2_improvement:.4f}")

        with col2:
            accuracy_improvement = (
                model_metrics["exact_accuracy"] - random_metrics["exact_accuracy"]
            )
            st.metric(
                "å®Œå…¨æ­£ç¢ºé æ¸¬ç‡æ”¹å–„",
                f"{accuracy_improvement:.2%}",
                delta=f"{accuracy_improvement:.2%}",
            )

        with col3:
            within_one_improvement = (
                model_metrics["within_one_accuracy"]
                - random_metrics["within_one_accuracy"]
            )
            st.metric(
                "èª¤å·®Â±1ç¯„åœå…§æ”¹å–„",
                f"{within_one_improvement:.2%}",
                delta=f"{within_one_improvement:.2%}",
            )

        with col4:
            error_reduction = (
                (random_metrics["mae"] - model_metrics["mae"])
                / random_metrics["mae"]
                * 100
            )
            st.metric(
                "å¹³å‡èª¤å·®æ¸›å°‘",
                f"{error_reduction:.1f}%",
                delta=f"{error_reduction:.1f}%",
            )

        # è¦–è¦ºåŒ–æ¯”è¼ƒ
        st.subheader("è¦–è¦ºåŒ–æ¯”è¼ƒ")

        # 1. é æ¸¬å€¼ vs å¯¦éš›å€¼æ•£é»åœ–
        fig = make_subplots(
            rows=2,
            cols=2,
            subplot_titles=(
                "Ridgeæ¨¡å‹é æ¸¬",
                "éš¨æ©Ÿé æ¸¬",
                "èª¤å·®åˆ†å¸ƒæ¯”è¼ƒ",
                "å„å“è³ªç­‰ç´šæº–ç¢ºç‡",
            ),
            specs=[
                [{"type": "scatter"}, {"type": "scatter"}],
                [{"type": "histogram"}, {"type": "bar"}],
            ],
        )

        # Ridgeæ¨¡å‹æ•£é»åœ–
        fig.add_trace(
            go.Scatter(
                x=self.y_test,
                y=model_predictions,
                mode="markers",
                name="Ridgeæ¨¡å‹",
                marker=dict(color="blue", opacity=0.6),
            ),
            row=1,
            col=1,
        )

        # éš¨æ©Ÿé æ¸¬æ•£é»åœ–
        fig.add_trace(
            go.Scatter(
                x=self.y_test,
                y=random_predictions,
                mode="markers",
                name="éš¨æ©Ÿé æ¸¬",
                marker=dict(color="red", opacity=0.6),
            ),
            row=1,
            col=2,
        )

        # èª¤å·®åˆ†å¸ƒæ¯”è¼ƒ
        model_errors = np.abs(self.y_test - model_predictions)
        random_errors = np.abs(self.y_test - random_predictions)

        fig.add_trace(
            go.Histogram(
                x=model_errors, name="Ridgeæ¨¡å‹èª¤å·®", marker_color="blue", opacity=0.7
            ),
            row=2,
            col=1,
        )

        fig.add_trace(
            go.Histogram(
                x=random_errors, name="éš¨æ©Ÿé æ¸¬èª¤å·®", marker_color="red", opacity=0.7
            ),
            row=2,
            col=1,
        )

        # å„å“è³ªç­‰ç´šæº–ç¢ºç‡
        quality_levels = sorted(self.y_test.unique())
        model_accuracies = []
        random_accuracies = []

        for level in quality_levels:
            mask = self.y_test == level
            if np.sum(mask) > 0:
                model_pred_rounded = np.round(model_predictions[mask]).astype(int)
                model_acc = accuracy_score(
                    self.y_test[mask].astype(int), model_pred_rounded
                )
                model_accuracies.append(model_acc)

                random_pred_rounded = np.round(random_predictions[mask]).astype(int)
                random_acc = accuracy_score(
                    self.y_test[mask].astype(int), random_pred_rounded
                )
                random_accuracies.append(random_acc)
            else:
                model_accuracies.append(0)
                random_accuracies.append(0)

        fig.add_trace(
            go.Bar(
                x=quality_levels,
                y=model_accuracies,
                name="Ridgeæ¨¡å‹",
                marker_color="blue",
            ),
            row=2,
            col=2,
        )

        fig.add_trace(
            go.Bar(
                x=quality_levels,
                y=random_accuracies,
                name="éš¨æ©Ÿé æ¸¬",
                marker_color="red",
            ),
            row=2,
            col=2,
        )

        fig.update_layout(
            height=800, title_text="æ¨¡å‹èˆ‡éš¨æ©Ÿé æ¸¬è©³ç´°æ¯”è¼ƒ", showlegend=True
        )

        st.plotly_chart(fig, use_container_width=True)

        # çµ±è¨ˆé¡¯è‘—æ€§åˆ†æ
        st.subheader("çµ±è¨ˆé¡¯è‘—æ€§åˆ†æ")

        better_predictions = np.sum(model_errors < random_errors)
        total_samples = len(model_errors)
        better_ratio = better_predictions / total_samples

        improvement = (
            (np.mean(random_errors) - np.mean(model_errors))
            / np.mean(random_errors)
            * 100
        )

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric(
                "æ¨¡å‹æ›´æº–ç¢ºçš„æ¨£æœ¬",
                f"{better_predictions}/{total_samples}",
                f"{better_ratio:.1%}",
            )

        with col2:
            st.metric("æ•´é«”æ”¹å–„å¹…åº¦", f"{improvement:.1f}%")

        with col3:
            st.metric(
                "å¹³å‡èª¤å·®æ¸›å°‘", f"{np.mean(random_errors) - np.mean(model_errors):.3f}"
            )

    def show_rfe_analysis(self):
        """é¡¯ç¤ºRFEåˆ†æçµæœ"""
        st.header("ğŸ” RFE (éæ­¸ç‰¹å¾µæ¶ˆé™¤) åˆ†æ")

        try:
            # è¼‰å…¥RFEçµæœ
            rfe_file = "rfe_results.csv"
            if os.path.exists(rfe_file):
                rfe_df = pd.read_csv(rfe_file)

                st.subheader("RFEçµæœè¡¨æ ¼")
                st.dataframe(rfe_df, use_container_width=True)

                # RFEè¦–è¦ºåŒ–
                st.subheader("RFEåˆ†æè¦–è¦ºåŒ–")

                fig = make_subplots(
                    rows=1,
                    cols=2,
                    subplot_titles=("R2 vs ç‰¹å¾µæ•¸é‡", "RMSE vs ç‰¹å¾µæ•¸é‡"),
                    specs=[[{"type": "scatter"}, {"type": "scatter"}]],
                )

                for model in rfe_df["Model"].unique():
                    model_data = rfe_df[rfe_df["Model"] == model]

                    fig.add_trace(
                        go.Scatter(
                            x=model_data["N_Features"],
                            y=model_data["Val_R2"],
                            mode="lines+markers",
                            name=f"{model} R2",
                            line=dict(width=2),
                        ),
                        row=1,
                        col=1,
                    )

                    fig.add_trace(
                        go.Scatter(
                            x=model_data["N_Features"],
                            y=model_data["Val_RMSE"],
                            mode="lines+markers",
                            name=f"{model} RMSE",
                            line=dict(width=2),
                        ),
                        row=1,
                        col=2,
                    )

                fig.update_layout(height=500, title_text="RFEåˆ†æçµæœ")

                st.plotly_chart(fig, use_container_width=True)

                # æœ€ä½³ç‰¹å¾µæ•¸é‡
                st.subheader("æœ€ä½³ç‰¹å¾µæ•¸é‡")
                best_features = rfe_df.loc[rfe_df["Val_R2"].idxmax()]
                st.metric("æœ€ä½³ç‰¹å¾µæ•¸é‡", best_features["N_Features"])
                st.metric("æœ€ä½³R2", f"{best_features['Val_R2']:.4f}")
                st.metric("å°æ‡‰RMSE", f"{best_features['Val_RMSE']:.4f}")

            else:
                st.warning("æ‰¾ä¸åˆ°RFEçµæœæª”æ¡ˆ")

        except Exception as e:
            st.error(f"è¼‰å…¥RFEçµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def show_feature_importance(self):
        """é¡¯ç¤ºç‰¹å¾µé‡è¦æ€§"""
        st.header("ğŸ“ˆ ç‰¹å¾µé‡è¦æ€§åˆ†æ")

        try:
            # è¼‰å…¥ç‰¹å¾µé‡è¦æ€§
            importance_file = "results/data/all_features_importance.csv"
            if os.path.exists(importance_file):
                importance_df = pd.read_csv(importance_file)

                st.subheader("ç‰¹å¾µé‡è¦æ€§è¡¨æ ¼")
                st.dataframe(importance_df, use_container_width=True)

                # ç‰¹å¾µé‡è¦æ€§è¦–è¦ºåŒ–
                st.subheader("ç‰¹å¾µé‡è¦æ€§è¦–è¦ºåŒ–")

                # å¹³å‡é‡è¦æ€§
                avg_importance = importance_df.set_index("Feature")[
                    "Average_Importance"
                ].sort_values(ascending=True)

                fig = px.bar(
                    x=avg_importance.values,
                    y=avg_importance.index,
                    orientation="h",
                    title="å¹³å‡ç‰¹å¾µé‡è¦æ€§",
                    color=avg_importance.values,
                    color_continuous_scale="viridis",
                )

                fig.update_layout(xaxis_title="é‡è¦æ€§", yaxis_title="ç‰¹å¾µ", height=500)

                st.plotly_chart(fig, use_container_width=True)

                # å„æ¨¡å‹ç‰¹å¾µé‡è¦æ€§æ¯”è¼ƒ
                st.subheader("å„æ¨¡å‹ç‰¹å¾µé‡è¦æ€§æ¯”è¼ƒ")

                model_columns = [
                    col
                    for col in importance_df.columns
                    if col not in ["Feature", "Average_Importance"]
                ]

                fig = go.Figure()

                for col in model_columns:
                    fig.add_trace(
                        go.Bar(
                            x=importance_df["Feature"],
                            y=importance_df[col],
                            name=col,
                            opacity=0.8,
                        )
                    )

                fig.update_layout(
                    title="å„æ¨¡å‹ç‰¹å¾µé‡è¦æ€§æ¯”è¼ƒ",
                    xaxis_title="ç‰¹å¾µ",
                    yaxis_title="é‡è¦æ€§",
                    barmode="group",
                    height=500,
                )

                st.plotly_chart(fig, use_container_width=True)

            else:
                st.warning("æ‰¾ä¸åˆ°ç‰¹å¾µé‡è¦æ€§æª”æ¡ˆ")

        except Exception as e:
            st.error(f"è¼‰å…¥ç‰¹å¾µé‡è¦æ€§æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def show_outlier_analysis(self):
        """é¡¯ç¤ºç•°å¸¸å€¼åˆ†æ"""
        st.header("ğŸ” ç•°å¸¸å€¼åˆ†æ")

        try:
            # è¼‰å…¥ç•°å¸¸å€¼åˆ†æçµæœ
            outlier_file = "processed_data/outliers_removed.csv"
            if os.path.exists(outlier_file):
                outliers_df = pd.read_csv(outlier_file)

                st.subheader("ç§»é™¤çš„ç•°å¸¸å€¼çµ±è¨ˆ")
                st.metric("ç§»é™¤çš„ç•°å¸¸å€¼æ•¸é‡", len(outliers_df))

                # ç•°å¸¸å€¼ç‰¹å¾µåˆ†æ
                st.subheader("ç•°å¸¸å€¼ç‰¹å¾µåˆ†æ")

                # æ¯”è¼ƒç•°å¸¸å€¼èˆ‡æ­£å¸¸å€¼çš„ç‰¹å¾µåˆ†å¸ƒ
                normal_data = self.processed_data["without_outliers"]

                comparison_features = [
                    "alcohol",
                    "volatile acidity",
                    "citric acid",
                    "residual sugar",
                ]

                fig = make_subplots(
                    rows=2,
                    cols=2,
                    subplot_titles=comparison_features,
                    specs=[
                        [{"type": "histogram"}, {"type": "histogram"}],
                        [{"type": "histogram"}, {"type": "histogram"}],
                    ],
                )

                for i, feature in enumerate(comparison_features):
                    row = i // 2 + 1
                    col = i % 2 + 1

                    fig.add_trace(
                        go.Histogram(
                            x=normal_data[feature],
                            name="æ­£å¸¸å€¼",
                            marker_color="blue",
                            opacity=0.7,
                        ),
                        row=row,
                        col=col,
                    )

                    fig.add_trace(
                        go.Histogram(
                            x=outliers_df[feature],
                            name="ç•°å¸¸å€¼",
                            marker_color="red",
                            opacity=0.7,
                        ),
                        row=row,
                        col=col,
                    )

                fig.update_layout(
                    height=600, title_text="ç•°å¸¸å€¼èˆ‡æ­£å¸¸å€¼ç‰¹å¾µåˆ†å¸ƒæ¯”è¼ƒ", showlegend=True
                )

                st.plotly_chart(fig, use_container_width=True)

                # ç•°å¸¸å€¼çµ±è¨ˆè¡¨æ ¼
                st.subheader("ç•°å¸¸å€¼çµ±è¨ˆ")
                outlier_stats = outliers_df.describe()
                st.dataframe(outlier_stats, use_container_width=True)

            else:
                st.warning("æ‰¾ä¸åˆ°ç•°å¸¸å€¼åˆ†ææª”æ¡ˆ")

        except Exception as e:
            st.error(f"è¼‰å…¥ç•°å¸¸å€¼åˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

    def run(self):
        """åŸ·è¡ŒStreamlitæ‡‰ç”¨ç¨‹å¼"""
        st.title("ğŸ· è‘¡è„é…’å“è³ªé æ¸¬æ¨¡å‹åˆ†æ")
        st.markdown("---")

        # è¼‰å…¥è³‡æ–™
        if not self.load_data():
            st.error("ç„¡æ³•è¼‰å…¥è³‡æ–™ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆè·¯å¾‘")
            return

        # å´é‚Šæ¬„å°èˆª
        st.sidebar.title("ğŸ“‹ åˆ†æé¸å–®")

        analysis_options = {
            "è³‡æ–™é è™•ç†": self.show_data_preprocessing,
            "æ¨¡å‹æ•ˆèƒ½": self.show_model_performance,
            "æ¨¡å‹ vs éš¨æ©Ÿé æ¸¬": self.show_model_vs_random,
            "RFEåˆ†æ": self.show_rfe_analysis,
            "ç‰¹å¾µé‡è¦æ€§": self.show_feature_importance,
            "ç•°å¸¸å€¼åˆ†æ": self.show_outlier_analysis,
        }

        selected_analysis = st.sidebar.selectbox(
            "é¸æ“‡åˆ†æé …ç›®", list(analysis_options.keys())
        )

        # åŸ·è¡Œé¸å®šçš„åˆ†æ
        analysis_options[selected_analysis]()

        # é è…³
        st.markdown("---")
        st.markdown(
            "**åˆ†æå®Œæˆæ™‚é–“:** " + pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
        )
        st.markdown("**è³‡æ–™ä¾†æº:** WineQT è³‡æ–™é›†")
        st.markdown("**åˆ†ææ–¹æ³•:** CRISP-DM æ¡†æ¶")


def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    app = StreamlitApp()
    app.run()


if __name__ == "__main__":
    main()
